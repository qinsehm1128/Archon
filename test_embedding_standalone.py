#!/usr/bin/env python3
"""
ç‹¬ç«‹çš„åµŒå…¥æ¨¡å‹æµ‹è¯•è„šæœ¬
è·³è¿‡æ‰€æœ‰æ•°æ®åº“å’Œé…ç½®æ£€æŸ¥ï¼Œç›´æ¥æµ‹è¯• Qwen3-Embedding-0.6B æ¨¡å‹
"""

import asyncio
import sys
import os
import traceback
from pathlib import Path

async def test_direct_model():
    """ç›´æ¥æµ‹è¯•æ¨¡å‹ï¼Œä¸ä¾èµ–é¡¹ç›®ä»£ç """
    print("ğŸ¤– Direct model test starting...")

    try:
        from transformers import AutoModel, AutoTokenizer
        import torch

        model_name = "Qwen/Qwen3-Embedding-0.6B"
        cache_dir = "./models"
        device = 'cuda' if torch.cuda.is_available() else 'cpu'

        print(f"ğŸ“¥ Loading {model_name}")
        print(f"ğŸ’¾ Cache: {cache_dir}")
        print(f"ğŸ–¥ï¸  Device: {device}")

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )

        model = AutoModel.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        ).to(device)

        model.eval()

        print("âœ… Model loaded successfully!")

        # æµ‹è¯•åµŒå…¥ç”Ÿæˆ
        test_texts = ["Hello world", "This is a test", "Machine learning"]

        with torch.no_grad():
            inputs = tokenizer(
                test_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            ).to(device)

            outputs = model(**inputs)

            # Mean pooling
            token_embeddings = outputs[0]
            attention_mask = inputs['attention_mask']
            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

            # Normalize
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

        print(f"âœ… Generated embeddings!")
        print(f"   - Input texts: {len(test_texts)}")
        print(f"   - Embedding shape: {embeddings.shape}")
        print(f"   - Sample embedding: {embeddings[0][:5].tolist()}")

        return True

    except Exception as e:
        print(f"âŒ Error: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("=" * 50)
    print("ğŸ¯ Qwen Embedding Standalone Test")
    print("=" * 50)

    os.environ['HF_HOME'] = './models'

    success = asyncio.run(test_direct_model())

    if success:
        print("\nğŸ‰ Test successful! Qwen model works!")
    else:
        print("\nâŒ Test failed!")

    exit(0 if success else 1)